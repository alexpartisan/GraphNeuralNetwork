
--- [START 2019-07-03_10-31-44] ----------------------------------------------------------------

	@train_gnn.py:  
	set random seed
		SEED = 1562121104
	set cuda environment
		torch.__version__              = 1.1.0
		torch.version.cuda             = 9.0.176
		torch.backends.cudnn.version() = 7501
		os['CUDA_VISIBLE_DEVICES']     = 0
		torch.cuda.device_count()      = 1



	SEED         = 1562121104
	PROJECT_PATH = /home/alexpartisan/Work/Data/Competition/CHAMP/GNN/build
	__file__     = /home/alexpartisan/Work/Data/Competition/CHAMP/GNN/build/train_gnn.py
	out_dir      = /home/alexpartisan/Work/Data/Competition/CHAMP/GNN/results/0703

** dataset setting **

--- [START 2019-07-03_10-32-04] ----------------------------------------------------------------

	@train_gnn.py:  
	set random seed
		SEED = 1562121124
	set cuda environment
		torch.__version__              = 1.1.0
		torch.version.cuda             = 9.0.176
		torch.backends.cudnn.version() = 7501
		os['CUDA_VISIBLE_DEVICES']     = 0
		torch.cuda.device_count()      = 1



	SEED         = 1562121124
	PROJECT_PATH = /home/alexpartisan/Work/Data/Competition/CHAMP/GNN/build
	__file__     = /home/alexpartisan/Work/Data/Competition/CHAMP/GNN/build/train_gnn.py
	out_dir      = /home/alexpartisan/Work/Data/Competition/CHAMP/GNN/results/0703

** dataset setting **
batch_size = 16
train_dataset : 
	mode   = train
	split  = train_split_by_mol.80003.npy
	csv    = train
	len    = 80003

valid_dataset : 
	mode   = train
	split  = valid_split_by_mol.5000.npy
	csv    = train
	len    = 5000


** net setting **
	initial_checkpoint = None
<class '__main__.Net'>

optimizer
  Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
schduler
  NullScheduler
lr=0.00100 

** start training here! **
   batch_size =16,  iter_accum=1
                      |--------------- VALID ----------------------------------------------------------------|-- TRAIN/BATCH ---------
                      |std 18.3     4.5     3.1    10.9     3.7     1.3     4.0    3.7  |                    |        | 
rate     iter   epoch |    1JHC,   2JHC,   3JHC,   1JHN,   2JHN,   3JHN,   2JHH,   3JHH |  loss  mae log_mae | loss   | time          
--------------------------------------------------------------------------------------------------------------------------------------
0.00000    0.0*   0.0 |  +4.551, +1.011, +1.310, +3.858, +1.188, +0.072, +2.355, +1.570 | +2.908 21.03 +1.99 | +0.000 |  0 hr 01 min
0.00100    0.5    0.1 |  +1.122, +0.410, +0.676, +1.418, +0.346, -0.184, +0.231, +0.771 | +0.671  2.04 +0.60 | +0.798 |  0 hr 04 min
0.00100    1.0    0.2 |  +0.796, +0.024, +0.465, +1.066, +0.155, -0.356, +0.017, +0.484 | +0.387  1.53 +0.33 | +0.469 |  0 hr 08 min
0.00100    1.5    0.3 |  +0.704, -0.060, +0.358, +0.848, -0.177, -0.402, -0.219, +0.168 | +0.249  1.28 +0.15 | +0.359 |  0 hr 12 min
0.00100    2.0    0.4 |  +0.652, -0.071, +0.229, +0.796, -0.322, -0.516, -0.299, -0.053 | +0.154  1.17 +0.05 | +0.212 |  0 hr 16 min
0.00100    2.5*   0.5 |  +0.588, -0.254, +0.173, +0.774, -0.359, -0.615, -0.381, +0.004 | +0.084  1.11 -0.01 | +0.170 |  0 hr 19 min
0.00100    3.0    0.6 |  +0.608, -0.323, +0.065, +0.828, -0.441, -0.676, -0.419, -0.238 | +0.010  1.06 -0.07 | +0.079 |  0 hr 23 min
0.00100    3.5    0.7 |  +0.457, -0.285, +0.016, +0.668, -0.392, -0.695, -0.475, -0.291 | -0.052  0.98 -0.12 | +0.058 |  0 hr 27 min
0.00100    4.0    0.8 |  +0.361, -0.236, -0.028, +0.750, -0.529, -0.741, -0.420, -0.320 | -0.081  0.97 -0.15 | +0.007 |  0 hr 31 min
0.00100    4.5    0.9 |  +0.320, -0.350, -0.075, +0.578, -0.544, -0.760, -0.500, -0.385 | -0.147  0.89 -0.21 | -0.004 |  0 hr 35 min
0.00100    5.0*   1.0 |  +0.515, -0.374, -0.057, +0.400, -0.549, -0.779, -0.511, -0.370 | -0.102  0.89 -0.22 | -0.067 |  0 hr 38 min
0.00100    5.5    1.1 |  +0.291, -0.446, -0.156, +0.464, -0.438, -0.751, -0.609, -0.447 | -0.214  0.84 -0.26 | -0.123 |  0 hr 42 min
0.00100    6.0    1.2 |  +0.275, -0.517, -0.185, +0.292, -0.627, -0.872, -0.677, -0.444 | -0.251  0.77 -0.34 | -0.143 |  0 hr 45 min
0.00100    6.5    1.3 |  +0.345, -0.419, -0.175, +0.400, -0.690, -0.860, -0.622, -0.270 | -0.190  0.83 -0.29 | -0.133 |  0 hr 48 min
0.00100    7.0    1.4 |  +0.265, -0.521, -0.213, +0.508, -0.688, -0.862, -0.705, -0.472 | -0.267  0.80 -0.34 | -0.191 |  0 hr 52 min
0.00100    7.5*   1.5 |  +0.209, -0.511, -0.236, +0.255, -0.720, -0.951, -0.719, -0.562 | -0.304  0.73 -0.40 | -0.272 |  0 hr 55 min
0.00100    8.0    1.6 |  +0.192, -0.534, -0.244, +0.339, -0.715, -0.832, -0.721, -0.553 | -0.312  0.75 -0.38 | -0.256 |  0 hr 59 min
0.00100    8.5    1.7 |  +0.211, -0.595, -0.299, +0.404, -0.766, -0.915, -0.761, -0.552 | -0.340  0.74 -0.41 | -0.208 |  1 hr 02 min
0.00100    9.0    1.8 |  +0.215, -0.419, -0.278, +0.339, -0.750, -0.853, -0.482, -0.571 | -0.282  0.77 -0.35 | -0.303 |  1 hr 05 min
0.00100    9.5    1.9 |  +0.190, -0.618, -0.278, +0.182, -0.809, -0.974, -0.630, -0.564 | -0.343  0.70 -0.44 | -0.299 |  1 hr 09 min
0.00100   10.0*   2.0 |  +0.130, -0.632, -0.319, +0.277, -0.820, -1.001, -0.735, -0.625 | -0.387  0.69 -0.47 | -0.316 |  1 hr 12 min
0.00100   10.5    2.1 |  +0.089, -0.616, -0.334, +0.386, -0.784, -0.978, -0.752, -0.582 | -0.392  0.71 -0.45 | -0.280 |  1 hr 16 min
0.00100   11.0    2.2 |  +0.217, -0.622, -0.327, +0.242, -0.864, -0.994, -0.739, -0.612 | -0.364  0.70 -0.46 | -0.305 |  1 hr 19 min
0.00100   11.5    2.3 |  +0.147, -0.642, -0.341, +0.250, -0.815, -0.967, -0.859, -0.631 | -0.398  0.68 -0.48 | -0.370 |  1 hr 23 min
0.00100   12.0    2.4 |  +0.133, -0.698, -0.371, +0.119, -0.858, -1.014, -0.854, -0.670 | -0.430  0.65 -0.53 | -0.359 |  1 hr 26 min
0.00100   12.5*   2.5 |  +0.149, -0.646, -0.391, +0.168, -0.816, -1.045, -0.800, -0.674 | -0.420  0.66 -0.51 | -0.333 |  1 hr 29 min
0.00100   13.0    2.6 |  +0.116, -0.667, -0.375, +0.164, -0.908, -1.004, -0.741, -0.546 | -0.411  0.67 -0.50 | -0.410 |  1 hr 33 min
0.00100   13.5    2.7 |  +0.165, -0.674, -0.381, +0.089, -0.921, -0.980, -0.842, -0.658 | -0.419  0.65 -0.53 | -0.324 |  1 hr 36 min
0.00100   14.0    2.8 |  +0.123, -0.718, -0.375, +0.123, -0.858, -1.000, -0.841, -0.616 | -0.431  0.65 -0.52 | -0.362 |  1 hr 40 min
0.00100   14.5    2.9 |  +0.378, -0.545, -0.218, +0.238, -0.751, -0.912, -0.684, -0.453 | -0.246  0.77 -0.37 | -0.264 |  1 hr 43 min
0.00100   15.0*   3.0 |  +0.113, -0.655, -0.369, +0.134, -0.863, -1.026, -0.833, -0.648 | -0.424  0.65 -0.52 | -0.368 |  1 hr 46 min
0.00100   15.5    3.1 |  +0.017, -0.747, -0.397, +0.139, -0.933, -1.035, -0.857, -0.747 | -0.487  0.62 -0.57 | -0.417 |  1 hr 50 min
0.00100   16.0    3.2 |  +0.064, -0.717, -0.414, +0.279, -0.927, -1.056, -0.887, -0.739 | -0.474  0.65 -0.55 | -0.435 |  1 hr 53 min
0.00100   16.5    3.3 |  +0.044, -0.690, -0.420, +0.061, -0.844, -1.091, -0.914, -0.709 | -0.478  0.62 -0.57 | -0.411 |  1 hr 57 min
0.00100   17.0    3.4 |  +0.174, -0.670, -0.377, +0.151, -0.796, -1.047, -0.807, -0.632 | -0.407  0.67 -0.50 | -0.277 |  2 hr 00 min
0.00100   17.5*   3.5 |  +0.043, -0.744, -0.446, +0.373, -0.928, -1.105, -0.922, -0.735 | -0.496  0.65 -0.56 | -0.436 |  2 hr 03 min
0.00100   18.0    3.6 |  +0.057, -0.715, -0.436, +0.103, -0.986, -1.085, -0.868, -0.690 | -0.482  0.62 -0.58 | -0.369 |  2 hr 07 min
0.00100   18.5    3.7 |  +0.039, -0.764, -0.467, +0.094, -0.895, -1.057, -0.932, -0.732 | -0.513  0.61 -0.59 | -0.462 |  2 hr 10 min
0.00100   19.0    3.8 |  +0.086, -0.682, -0.460, +0.149, -1.000, -1.109, -0.831, -0.701 | -0.476  0.63 -0.57 | -0.450 |  2 hr 14 min
0.00100   19.5    3.9 |  -0.032, -0.755, -0.487, +0.015, -0.995, -1.140, -0.964, -0.720 | -0.542  0.58 -0.63 | -0.462 |  2 hr 17 min
0.00100   20.0*   4.0 |  -0.040, -0.782, -0.474, -0.029, -0.977, -1.087, -0.972, -0.779 | -0.551  0.57 -0.64 | -0.497 |  2 hr 20 min
0.00100   20.5    4.1 |  -0.024, -0.794, -0.476, +0.156, -1.050, -1.031, -0.959, -0.793 | -0.548  0.60 -0.62 | -0.499 |  2 hr 24 min
0.00100   21.0    4.2 |  -0.024, -0.765, -0.450, +0.019, -0.955, -1.059, -0.866, -0.761 | -0.524  0.59 -0.61 | -0.483 |  2 hr 27 min
0.00100   21.5    4.3 |  -0.022, -0.814, -0.495, +0.033, -1.015, -1.113, -0.995, -0.811 | -0.563  0.57 -0.65 | -0.525 |  2 hr 31 min
0.00100   22.0    4.4 |  +0.007, -0.723, -0.455, +0.081, -0.954, -1.106, -0.822, -0.680 | -0.501  0.61 -0.58 | -0.464 |  2 hr 34 min
0.00100   22.5*   4.5 |  -0.093, -0.790, -0.515, +0.352, -1.067, -1.148, -1.006, -0.858 | -0.585  0.60 -0.64 | -0.504 |  2 hr 38 min

--- [START 2019-07-03_16-49-50] ----------------------------------------------------------------

	@train_gnn.py:  
	set random seed
		SEED = 1562143790
	set cuda environment
		torch.__version__              = 1.1.0
		torch.version.cuda             = 9.0.176
		torch.backends.cudnn.version() = 7501
		os['CUDA_VISIBLE_DEVICES']     = 0
		torch.cuda.device_count()      = 1



	SEED         = 1562143790
	PROJECT_PATH = /home/alexpartisan/Work/Data/Competition/CHAMP/GNN/build
	__file__     = /home/alexpartisan/Work/Data/Competition/CHAMP/GNN/build/train_gnn.py
	out_dir      = /home/alexpartisan/Work/Data/Competition/CHAMP/GNN/results/0703

** dataset setting **
batch_size = 16
train_dataset : 
	mode   = train
	split  = train_split_by_mol.80003.npy
	csv    = train
	len    = 80003

valid_dataset : 
	mode   = train
	split  = valid_split_by_mol.5000.npy
	csv    = train
	len    = 5000


** net setting **
	initial_checkpoint = None
<class '__main__.Net'>

optimizer
  Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
schduler
  NullScheduler
lr=0.00100 

** start training here! **
   batch_size =16,  iter_accum=1
                      |--------------- VALID ----------------------------------------------------------------|-- TRAIN/BATCH ---------
                      |std 18.3     4.5     3.1    10.9     3.7     1.3     4.0    3.7  |                    |        | 
rate     iter   epoch |    1JHC,   2JHC,   3JHC,   1JHN,   2JHN,   3JHN,   2JHH,   3JHH |  loss  mae log_mae | loss   | time          
--------------------------------------------------------------------------------------------------------------------------------------
